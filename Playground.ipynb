{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import traceback\n",
    "import torch\n",
    "import mne\n",
    "import scipy\n",
    "import json\n",
    "import torchvision\n",
    "import multiprocessing\n",
    "\n",
    "import scipy.signal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class dotdict(dict):\n",
    "\t\"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "\t__getattr__ = dict.get\n",
    "\t__setattr__ = dict.__setitem__\n",
    "\t__delattr__ = dict.__delitem__\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path config\n",
    "\n",
    "DATA_PATH = 'Cleared'\n",
    "OUT_PATH  = f'{DATA_PATH}-converted'\n",
    "CLEARED_PATH   = OUT_PATH\n",
    "VISUAL_SUBPATH = 'Visual'\n",
    "AUDIAL_SUBPATH = 'Audial'\n",
    "\n",
    "# Data config\n",
    "# 2X - internal speaking\n",
    "# 1X - stimulus\n",
    "# Phoneme length ~300ms\n",
    "\n",
    "TARGET_CHANNELS = 4\n",
    "TARGET_CHANNEL_SETS = [\n",
    "\t[ 'EEG F7-A1', 'EEG F7-M1' ],\n",
    "\t[ 'EEG F3-A1', 'EEG F3-M1' ],\n",
    "\t[ 'EEG T3-A1', 'EEG T3-M1' ],\n",
    "\t[ 'EEG C3-A1', 'EEG C3-M1' ],\n",
    "]\n",
    "SOURCE_FREQ     = 1000 # Article: 1000Hz\n",
    "SECTOR_LENGTH   = 600\n",
    "SECTOR_LENGTH_STEPS = 600\n",
    "MAX_MORLET_FREQ = 30\n",
    "MORLET_FREQ_STEPS = 30\n",
    "LOW_PASS_FREQ   = 3\n",
    "HIGH_PASS_FREQ  = 30\n",
    "MAX_SAMPLE_LENGTH = 1.5\n",
    "\n",
    "# Phonemes are enumerated in range 2, 3, 4, 5, 6, 7, 8\n",
    "MIN_PHONEME_ID = 2\n",
    "PHONEME_COUNT  = 7\n",
    "\n",
    "# Directories\n",
    "MORLET_ORIGINAL_SAVE_DIR = 'morlet-original'\n",
    "\n",
    "# List of EDF files to use\n",
    "# These files are taken from CLEARED_PATH/VISUAL_SUBPATH and CLEARED_PATH/AUDIAL_SUBPATH\n",
    "INPUT_EDF_LIST = [\n",
    "\t'Antonovazrf_och',\n",
    "\t'BazvlkDzrf_och',\n",
    "\t'DachaPapzrf_och',\n",
    "\t'Drachenkozrf_och',\n",
    "\t'Gordokovzrf_och',\n",
    "\t'Manenkovzrf_och',\n",
    "\t'pavluhinNzrf_och',\n",
    "\t'rylkovSzrf_och',\n",
    "\t'Sazanovazrf_och',\n",
    "\t'vinickiDzrf_och',\n",
    "]\n",
    "\n",
    "\n",
    "# Flags\n",
    "CONVERT_EDF = False\n",
    "CONVERT_MORLETS = False\n",
    "ENABLE_DEMO = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > Convert all files from custom EDF to normal EDF\n",
    "\n",
    "**WARNING: Remove headers for each EDF file via EDFBrowser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_edf():\n",
    "\t\"\"\"\n",
    "\tConvert EDF files using Mitr_Edf.exe utility.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tos.makedirs(f'{OUT_PATH}/{VISUAL_SUBPATH}', exist_ok=True)\n",
    "\tos.makedirs(f'{OUT_PATH}/{AUDIAL_SUBPATH}', exist_ok=True)\n",
    "\n",
    "\tfor file in os.listdir(f'{DATA_PATH}/{VISUAL_SUBPATH}'):\n",
    "\t\tprocessed_file = f'{DATA_PATH}/{VISUAL_SUBPATH}/{file[:-4]}_.EDF'\n",
    "\t\ttarget_file    =  f'{OUT_PATH}/{VISUAL_SUBPATH}/{file}'\n",
    "\t\tfile           = f'{DATA_PATH}/{VISUAL_SUBPATH}/{file}'\n",
    "\t\t\n",
    "\t\tprint('Processing', file)\n",
    "\t\ttry:\n",
    "\t\t\tos.remove(processed_file)\n",
    "\t\texcept:\n",
    "\t\t\tpass\n",
    "\t\tsubprocess.run(f'Mitr_Edf.exe {file}')\n",
    "\t\t\n",
    "\t\tprint('Moving', processed_file, 'to', target_file)\n",
    "\t\ttry:\n",
    "\t\t\tos.rename(processed_file, target_file)\n",
    "\t\texcept:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tos.replace(processed_file, target_file)\n",
    "\t\t\texcept:\n",
    "\t\t\t\tprint('Critical failture')\n",
    "\t\t\t\ttraceback.print_exc()\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\tfor file in os.listdir(f'{DATA_PATH}/{AUDIAL_SUBPATH}'):\n",
    "\t\tprocessed_file = f'{DATA_PATH}/{AUDIAL_SUBPATH}/{file[:-4]}_.EDF'\n",
    "\t\ttarget_file    =  f'{OUT_PATH}/{AUDIAL_SUBPATH}/{file}'\n",
    "\t\tfile           = f'{DATA_PATH}/{AUDIAL_SUBPATH}/{file}'\n",
    "\t\t\n",
    "\t\tprint('Processing', file)\n",
    "\t\ttry:\n",
    "\t\t\tos.remove(processed_file)\n",
    "\t\texcept:\n",
    "\t\t\tpass\n",
    "\t\tsubprocess.run(f'Mitr_Edf.exe {file}')\n",
    "\t\t\n",
    "\t\tprint('Moving', processed_file, 'to', target_file)\n",
    "\t\ttry:\n",
    "\t\t\tos.rename(processed_file, target_file)\n",
    "\t\texcept:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tos.replace(processed_file, target_file)\n",
    "\t\t\texcept:\n",
    "\t\t\t\tprint('Critical failture')\n",
    "\t\t\t\ttraceback.print_exc()\n",
    "\t\t\t\tbreak"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > Basic functions for data processing & train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read EDF files, extract, split into sectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sectors(edf): # Returns sectors[begin,end] and missing_labels\n",
    "\t\"\"\"\n",
    "\tExtract valid and invalid sectors from edf data file\n",
    "\t\"\"\"\n",
    "\tsectors = []\n",
    "\tmissing_labels = []\n",
    "\tlast_label = None\n",
    "\tlast_label_index = None\n",
    "\t\n",
    "\tMETKA = edf['METKA']\n",
    "\tX = METKA[1]\n",
    "\tY = METKA[0].T[:,0]\n",
    "\n",
    "\tfor index, (timestamp, value) in enumerate(zip(X, Y)):\n",
    "\t\tif value > 0:\n",
    "\t\t\tvalue = int(value)\n",
    "\t\t\t\n",
    "\t\t\t# Phoneme begin\n",
    "\t\t\tif value // 10 == 1:\n",
    "\t\t\t\tif last_label is not None:\n",
    "\t\t\t\t\tif last_label // 10 == 1:\n",
    "\t\t\t\t\t\tmissing_labels.append(last_label_index)\n",
    "\t\t\t\t\n",
    "\t\t\t\tlast_label = value\n",
    "\t\t\t\tlast_label_index = index\n",
    "\t\t\t\n",
    "\t\t\t# Phoneme end\n",
    "\t\t\telif value // 10 == 2:\n",
    "\t\t\t\tif last_label is not None:\n",
    "\t\t\t\t\tif last_label % 10 != value % 10:\n",
    "\t\t\t\t\t\tmissing_labels.append(last_label_index)\n",
    "\t\t\t\t\t\tmissing_labels.append(index)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tsectors.append((last_label_index, index))\n",
    "\t\t\t\t\t\tlast_label = None\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tmissing_labels.append(index)\n",
    "\t\n",
    "\treturn sectors, missing_labels\n",
    "\n",
    "def extract_strict_sectors(edf, sector_length): # Returns sectors[begin,end] and labels\n",
    "\t\"\"\"\n",
    "\tExtract sectors of the given length using begin and start labels.\n",
    "\tSample usage is extracting sectors of length 600 ms (1000Hz).\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tsectors = []\n",
    "\tlabels = []\n",
    "\tinvalid_labels = []\n",
    "\tlast_sector_end_index = None\n",
    "\t\n",
    "\tMETKA = edf['METKA']\n",
    "\tX = METKA[1]\n",
    "\tY = METKA[0].T[:,0]\n",
    "\n",
    "\tfor index, (timestamp, value) in enumerate(zip(X, Y)):\n",
    "\t\tif value > 0:\n",
    "\t\t\t\n",
    "\t\t\tif last_sector_end_index is not None and index < last_sector_end_index:\n",
    "\t\t\t\tinvalid_labels.append(index)\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t\n",
    "\t\t\t# Assume that sector [index : index+sector_length] does not \n",
    "\t\t\t#  intersect with other sector [index2 : index2+sector_length]\n",
    "\t\t\tvalue = int(value)\n",
    "\t\t\t\n",
    "\t\t\t# Phoneme begin\n",
    "\t\t\tif value // 10 == 1:\n",
    "\t\t\t\t\n",
    "\t\t\t\tif index + sector_length > len(X):\n",
    "\t\t\t\t\tinvalid_labels.append(index)\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Append sector from current position to current+sector_length as sector\n",
    "\t\t\t\tsectors.append((index, index + sector_length))\n",
    "\t\t\t\tlabels.append(value % 10)\n",
    "\t\t\t\tlast_sector_end_index = index + sector_length\n",
    "\t\t\t\n",
    "\t\t\t# Phoneme end\n",
    "\t\t\telif value // 10 == 2:\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Ignore underflow\n",
    "\t\t\t\tif index - sector_length < 0:\n",
    "\t\t\t\t\tinvalid_labels.append(index)\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Append sector from current position to current+sector_length as sector\n",
    "\t\t\t\tsectors.append((index - sector_length, index))\n",
    "\t\t\t\tlabels.append(value % 10)\n",
    "\t\t\t\tlast_sector_end_index = index\n",
    "\t\n",
    "\treturn sectors, invalid_labels, labels\n",
    "\n",
    "def extract_strict_sectors_with_offset(edf, sector_length, first_label_offset): # Returns sectors[begin,end] and labels\n",
    "\t\"\"\"\n",
    "\tExtract sectors of the given length using begin and start labels.\n",
    "\tSample usage is extracting sectors of length 600 ms (1000Hz).\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tsectors = []\n",
    "\tlabels = []\n",
    "\tinvalid_labels = []\n",
    "\tlast_sector_end_index = None\n",
    "\t\n",
    "\tMETKA = edf['METKA']\n",
    "\tX = METKA[1]\n",
    "\tY = METKA[0].T[:,0]\n",
    "\n",
    "\tfor index, (timestamp, value) in enumerate(zip(X, Y)):\n",
    "\t\tif value > 0:\n",
    "\t\t\t\n",
    "\t\t\tif last_sector_end_index is not None and index < last_sector_end_index:\n",
    "\t\t\t\tinvalid_labels.append(index)\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t\n",
    "\t\t\t# Assume that sector [index : index+sector_length] does not \n",
    "\t\t\t#  intersect with other sector [index2 : index2+sector_length]\n",
    "\t\t\tvalue = int(value)\n",
    "\t\t\t\n",
    "\t\t\t# Phoneme begin\n",
    "\t\t\tif value // 10 == 1:\n",
    "\t\t\t\t\n",
    "\t\t\t\tif index + sector_length > len(X):\n",
    "\t\t\t\t\tinvalid_labels.append(index)\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Append sector from current position to current+sector_length as sector\n",
    "\t\t\t\tsectors.append((index + first_label_offset, index + first_label_offset + sector_length))\n",
    "\t\t\t\tlabels.append(value % 10)\n",
    "\t\t\t\tlast_sector_end_index = index + first_label_offset + sector_length\n",
    "\t\t\t\n",
    "\t\t\t# Phoneme end\n",
    "\t\t\telif value // 10 == 2:\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Ignore underflow\n",
    "\t\t\t\tif index - sector_length < 0:\n",
    "\t\t\t\t\tinvalid_labels.append(index)\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Append sector from current position to current+sector_length as sector\n",
    "\t\t\t\tsectors.append((index - sector_length, index))\n",
    "\t\t\t\tlabels.append(value % 10)\n",
    "\t\t\t\tlast_sector_end_index = index\n",
    "\t\n",
    "\treturn sectors, invalid_labels, labels\n",
    "\n",
    "def print_sectors_summary(edf, sectors, missing_labels):\n",
    "\t\"\"\"\n",
    "\tPrint summary info about given set of sectors\n",
    "\t\"\"\"\n",
    "\tprint('sectors:', len(sectors))\n",
    "\tprint('invalid:', len(missing_labels))\n",
    "\t\n",
    "\tMETKA = edf['METKA']\n",
    "\tX = METKA[1]\n",
    "\t\n",
    "\tdiff = np.array([ X[b] - X[a] for (a, b) in sectors ])\n",
    "\n",
    "\tprint('min sector length:', np.min(diff))\n",
    "\tprint('max sector length:', np.max(diff))\n",
    "\tprint('avg sector length:', np.average(diff))\n",
    "\n",
    "def plot_labels(edf, missing_labels):\n",
    "\t\"\"\"\n",
    "\tPlot distribution of valid and invalid labels\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tMETKA = edf['METKA']\n",
    "\tX = METKA[1]\n",
    "\tY = METKA[0].T[:,0]\n",
    "\t\n",
    "\tplt.rcParams[\"figure.figsize\"] = (25, 5)\n",
    "\tplt.rcParams[\"font.size\"] = 14\n",
    "\n",
    "\tfor index in range(len(X)):\n",
    "\t\tif Y[index] > 0:\n",
    "\t\t\tif index in missing_labels:\n",
    "\t\t\t\tplt.scatter(X[index], Y[index], color='red', marker='x')\n",
    "\t\t\telse:\n",
    "\t\t\t\tplt.scatter(X[index], Y[index], color='blue', marker='.')\n",
    "\n",
    "\tplt.show()\n",
    "\n",
    "def plot_sectors(sectors, missing_labels):\n",
    "\t\"\"\"\n",
    "\tPlot segments on single line, inclusing invalid sectors\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tplt.rcParams[\"figure.figsize\"] = (25, 2.5)\n",
    "\tplt.rcParams[\"font.size\"] = 14\n",
    "\n",
    "\t# Plot correct labels\n",
    "\tfor index, sector in enumerate(sectors):\n",
    "\t\tplt.plot(sector, (0, 0), color='blue', marker='|', label='sectors' if index == 0 else None)\n",
    "\n",
    "\t# plot invalid labels\n",
    "\tfor index, miss in enumerate(missing_labels):\n",
    "\t\tplt.scatter(miss, 0, color='red', marker='|', label='invalid sectors' if index == 0 else None)\n",
    "\n",
    "\tplt.legend()\n",
    "\tplt.show()\n",
    "\n",
    "def list_visual_edf():\n",
    "\t\"\"\"\n",
    "\tList visual EDF file names\n",
    "\t\"\"\"\n",
    "\t\n",
    "\treturn os.listdir(f'{OUT_PATH}/{VISUAL_SUBPATH}')\n",
    "\n",
    "def open_visual_edf(filename):\n",
    "\t\"\"\"\n",
    "\tOpen visual data file and return EDF object\n",
    "\t\"\"\"\n",
    "\tfile = f'{OUT_PATH}/{VISUAL_SUBPATH}/{filename}'\n",
    "\n",
    "\treturn mne.io.read_raw_edf(file)\n",
    "\n",
    "def list_audial_edf():\n",
    "\t\"\"\"\n",
    "\tList audial EDF file names\n",
    "\t\"\"\"\n",
    "\t\n",
    "\treturn os.listdir(f'{OUT_PATH}/{AUDIAL_SUBPATH}')\n",
    "\n",
    "def open_audial_edf(filename):\n",
    "\t\"\"\"\n",
    "\tOpen audial data file and return EDF object\n",
    "\t\"\"\"\n",
    "\tfile = f'{OUT_PATH}/{AUDIAL_SUBPATH}/{filename}'\n",
    "\n",
    "\treturn mne.io.read_raw_edf(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel extraction and wavelet pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subselect_channels(edf):\n",
    "\tprint(f'Available channels: {edf.ch_names}')\n",
    "\t\n",
    "\tchannels = [ None ] * TARGET_CHANNELS\n",
    "\tfor i in range(TARGET_CHANNELS):\n",
    "\t\t\n",
    "\t\t# Iterate over all channels find compatible channel names\n",
    "\t\tfor comatible in range (len(TARGET_CHANNEL_SETS[i])):\n",
    "\t\t\ttry:\n",
    "\t\t\t\tchannels[i] = edf[TARGET_CHANNEL_SETS[i][comatible]][0][0]\n",
    "\t\t\t\tbreak\n",
    "\t\t\texcept:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\n",
    "\t\tif channels[i] is None:\n",
    "\t\t\traise RuntimeError(f'No compatible channels found for channels {TARGET_CHANNEL_SETS[i]}')\n",
    "\t\n",
    "\treturn channels\n",
    "\n",
    "def butterworth_filter_pass(edf, channels_data):\n",
    "\t\n",
    "\tfiltered = [ None ] * TARGET_CHANNELS\n",
    "\t\n",
    "\tfor index, cd in enumerate(channels_data):\n",
    "\t\tfiltered[index] = mne.filter.filter_data(cd, SOURCE_FREQ, LOW_PASS_FREQ, HIGH_PASS_FREQ, method='iir')\n",
    "\t\n",
    "\treturn filtered\n",
    "\n",
    "def split_sectors(edf, channels_data, sectors):\n",
    "\t\"\"\"\n",
    "\tPerforms slicing of the given channels using sector info data.\n",
    "\tReturns label number, split length, split duration and splitted data for channels\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tMETKA = edf['METKA']\n",
    "\tX = METKA[1]\n",
    "\tY = METKA[0][0] # .T[:,0]\n",
    "\t\n",
    "\tsplitted  = [ [ None ] * len(sectors) for i in range(len(channels_data)) ]\n",
    "\tlengths   = [ None ] * len(sectors)\n",
    "\tdurations = [ None ] * len(sectors)\n",
    "\tlabels    = [ None ] * len(sectors)\n",
    "\t\n",
    "\tfor index in range(len(sectors)):\n",
    "\t\t(a, b) = sectors[index]\n",
    "\t\t\n",
    "\t\tlabels[index]    = int(Y[a]) % 10\n",
    "\t\tlengths[index]   = b - a\n",
    "\t\tdurations[index] = X[b] - X[a]\n",
    "\t\t\n",
    "\t\tfor index2, f in enumerate(channels_data):\n",
    "\t\t\tsplitted[index2][index] = f[a:b]\n",
    "\t\n",
    "\treturn labels, lengths, durations, splitted\n",
    "\n",
    "def single_morlet_wavelet_pass(sample, w = 6.):\n",
    "\t\"\"\"\n",
    "\tApply wavelet transform on the givven sample\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tt, dt = np.linspace(0, SECTOR_LENGTH / SOURCE_FREQ, SECTOR_LENGTH, retstep=True)\n",
    "\tfreq = np.linspace(1, MAX_MORLET_FREQ, MAX_MORLET_FREQ)\n",
    "\tfs = 1 / dt\n",
    "\twidths = w * fs / (2 * freq * np.pi)\n",
    "\t\n",
    "\treturn t[::SECTOR_LENGTH//SECTOR_LENGTH_STEPS], freq[::MAX_MORLET_FREQ//MORLET_FREQ_STEPS], scipy.signal.cwt(sample, scipy.signal.morlet2, widths, w=w)[::MAX_MORLET_FREQ//MORLET_FREQ_STEPS,::SECTOR_LENGTH//SECTOR_LENGTH_STEPS]\n",
    "\n",
    "def rescale_morlet_plz(sample):\n",
    "\t\"\"\"\n",
    "\tRescale from shape (MAX_MORLET_FREQ, SECTOR_LENGTH) to shape \n",
    "\t(MORLET_FREQ_STEPS, SECTOR_LENGTH_STEPS)\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tNW = SECTOR_LENGTH_STEPS\n",
    "\tFW = (SECTOR_LENGTH // SECTOR_LENGTH_STEPS)\n",
    "\t\n",
    "\tif SECTOR_LENGTH == SECTOR_LENGTH_STEPS:\n",
    "\t\treturn sample\n",
    "\t\n",
    "\tsample = np.reshape(sample, (MAX_MORLET_FREQ, NW, FW)).mean(axis=2)\n",
    "\t\n",
    "\tif MAX_MORLET_FREQ != MORLET_FREQ_STEPS:\n",
    "\t\traise RuntimeError('Incomplete code')\n",
    "\t\n",
    "\treturn sample\n",
    "\n",
    "def morlet_wavelet_pass(channel_splitted_data, w = 6.):\n",
    "\t\"\"\"\n",
    "\tPerforms wavelet transform over the given data. Returns 2D matrixes \n",
    "\trepresenting morlet transform application result for each of 4 channels for \n",
    "\teach of N samples.\n",
    "\t\n",
    "\tchannel_splitted_data contains 4 channels, each has a set of splitted \n",
    "\tsamples in it.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tt, dt = np.linspace(0, SECTOR_LENGTH / SOURCE_FREQ, SECTOR_LENGTH, retstep=True)\n",
    "\tfreq = np.linspace(1, MAX_MORLET_FREQ, MAX_MORLET_FREQ)\n",
    "\tfs = 1 / dt\n",
    "\twidths = w * fs / (2 * freq * np.pi)\n",
    "\t# [::MAX_MORLET_FREQ//MORLET_FREQ_STEPS,::SECTOR_LENGTH//SECTOR_LENGTH_STEPS]\n",
    "\t\n",
    "\t# NW = SECTOR_LENGTH_STEPS\n",
    "\t# np.reshape(scipy.signal.cwt(channel_splitted_data[channel][index], scipy.signal.morlet2, widths, w=w), (NW, FW, FH, -1)).mean(axis=3).mean(axis=1)\n",
    "\t# \tscipy.signal.cwt(channel_splitted_data[channel][index], scipy.signal.morlet2, widths, w=w)\n",
    "\t# if SECTOR_LENGTH == SECTOR_LENGTH_STEPS else\n",
    "\t# \tnp.reshape(scipy.signal.cwt(channel_splitted_data[channel][index], scipy.signal.morlet2, widths, w=w), (NW, FW, FH, -1)).mean(axis=3).mean(axis=1)\n",
    "\t# scipy.signal.cwt(channel_splitted_data[channel][index], scipy.signal.morlet2, widths, w=w)[::MAX_MORLET_FREQ//MORLET_FREQ_STEPS,::SECTOR_LENGTH//SECTOR_LENGTH_STEPS]\n",
    "\t\n",
    "\tFW = (MAX_MORLET_FREQ // MORLET_FREQ_STEPS)\n",
    "\tFH = (SECTOR_LENGTH // SECTOR_LENGTH_STEPS)\n",
    "\t\n",
    "\treturn t[::FH], freq[::FW], [ \n",
    "\t\t[\n",
    "\t\t\trescale_morlet_plz(scipy.signal.cwt(channel_splitted_data[channel][index], scipy.signal.morlet2, widths, w=w))\n",
    "\t\tfor index in range(len(channel_splitted_data[channel]))\n",
    "\t\t]\n",
    "\tfor channel in range(4)\n",
    "\t]\n",
    "\n",
    "def transpose_morlet_channel_data(morlet_channel_data):\n",
    "\t\"\"\"\n",
    "\tPerform transposition of channel data so order changes from\n",
    "\t\n",
    "\tmorlet_channel_data[channel][index]\n",
    "\t\n",
    "\tto\n",
    "\t\n",
    "\tmorlet_channel_data[index][channel]\n",
    "\t\"\"\"\n",
    "\t\n",
    "\treturn [\n",
    "\t\t[\n",
    "\t\t\tmorlet_channel_data[channel][index]\n",
    "\t\tfor channel in range(TARGET_CHANNELS)\n",
    "\t\t]\n",
    "\tfor index in range(len(morlet_channel_data[0]))\n",
    "\t]\n",
    "\n",
    "def abs_morlet_data(morlet):\n",
    "\treturn np.abs(morlet)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data save & load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_morlet(filename, morlet):\n",
    "\t\"\"\"\n",
    "\tWrite numpy morlet data to file\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tnp.save(filename, morlet)\n",
    "\n",
    "def read_morlet(filename):\n",
    "\t\"\"\"\n",
    "\tRead numpy morlet data from file\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tif os.path.exists(filename):\n",
    "\t\treturn np.load(filename)\n",
    "\t\n",
    "def auto_save_morlet(directory, person, phoneme, channel, sample, phoneme_data):\n",
    "\t\"\"\"\n",
    "\tWrite phoneme data to file with the following parameters:\n",
    "\t\n",
    "\tdirectory - directory to place files in\n",
    "\t\n",
    "\tperson - index of person / edf file\n",
    "\t\n",
    "\tphoneme - index of phoneme\n",
    "\t\n",
    "\tchannel - index of used channel\n",
    "\t\n",
    "\tsample - index of this phoneme's sample\n",
    "\t\n",
    "\tphoneme_data - phoneme morlet data\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tos.makedirs(directory, exist_ok=True)\n",
    "\t\n",
    "\tsave_morlet(f'{directory}/morlet_{person}_{phoneme}_{channel}_{sample}.npy', phoneme_data)\n",
    "\n",
    "def auto_load_morlet(directory, person, phoneme, channel, sample):\n",
    "\t\"\"\"\n",
    "\tRead morlet numpy data in the same way as auto_save_morlet()\n",
    "\t\"\"\"\n",
    "\t\n",
    "\treturn read_morlet(f'{directory}/morlet_{person}_{phoneme}_{channel}_{sample}.npy')\n",
    "\n",
    "def get_morlet_count(directory, person, phoneme):\n",
    "\t\"\"\"\n",
    "\tGet count of morlet samples for given person and phoneme ID\n",
    "\t\"\"\"\n",
    "\t\n",
    "\ttry:\n",
    "\t\twith open(f'{directory}/count.json', 'r') as f:\n",
    "\t\t\tdata = json.load(f)\n",
    "\t\t\t\n",
    "\t\t\treturn data[f'person_{person}'][f'phoneme_{phoneme}']\n",
    "\texcept:\n",
    "\t\treturn 0\n",
    "\n",
    "def set_morlet_count(directory, person, phoneme, count):\n",
    "\t\"\"\"\n",
    "\tGet count of morlet samples for given person and phoneme ID\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tos.makedirs(directory, exist_ok=True)\n",
    "\t\n",
    "\ttry:\n",
    "\t\twith open(f'{directory}/count.json', 'r') as f:\n",
    "\t\t\tdata = json.load(f)\n",
    "\texcept:\n",
    "\t\tdata = {}\n",
    "\t\t\t\n",
    "\tif f'person_{person}' not in data:\n",
    "\t\tdata[f'person_{person}'] = {}\n",
    "\t\n",
    "\tdata[f'person_{person}'][f'phoneme_{phoneme}'] = count\n",
    "\t\n",
    "\twith open(f'{directory}/count.json', 'w') as f:\n",
    "\t\tjson.dump(data, f)\n",
    "\n",
    "def get_total_morlet_count(directory, person):\n",
    "\t\"\"\"\n",
    "\tGet count of morlet samples for given person ID\n",
    "\t\"\"\"\n",
    "\t\n",
    "\ttry:\n",
    "\t\twith open(f'{directory}/count.json', 'r') as f:\n",
    "\t\t\tdata = json.load(f)\n",
    "\t\t\t\n",
    "\t\t\treturn data[f'person_{person}'][f'total']\n",
    "\texcept:\n",
    "\t\treturn 0\n",
    "\n",
    "def set_total_morlet_count(directory, person, count):\n",
    "\t\"\"\"\n",
    "\tGet count of morlet samples for given person ID\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tos.makedirs(directory, exist_ok=True)\n",
    "\t\n",
    "\ttry:\n",
    "\t\twith open(f'{directory}/count.json', 'r') as f:\n",
    "\t\t\tdata = json.load(f)\n",
    "\texcept:\n",
    "\t\tdata = {}\n",
    "\t\t\t\n",
    "\tif f'person_{person}' not in data:\n",
    "\t\tdata[f'person_{person}'] = {}\n",
    "\t\n",
    "\tdata[f'person_{person}'][f'total'] = count\n",
    "\t\n",
    "\twith open(f'{directory}/count.json', 'w') as f:\n",
    "\t\tjson.dump(data, f)\n",
    "\n",
    "def normalize_labels(labels):\n",
    "\t\"\"\"\n",
    "\tNormalize label values.\n",
    "\t\n",
    "\tSource label values start ffrom MIN_PHONEME_ID, normalization substracts \n",
    "\tMIN_PHONEME_ID from each phoneme ID.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\treturn [ p - MIN_PHONEME_ID for p in labels ]\n",
    "\n",
    "def group_morlet_by_phoneme(normalized_morlet_labels, morlet_list):\n",
    "\t\"\"\"\n",
    "\tGroup morlet data by phoneme ID  \n",
    "\t\"\"\"\n",
    "\t\n",
    "\tresult = [ [] for _ in range(PHONEME_COUNT) ]\n",
    "\tfor label, morlet in zip(normalized_morlet_labels, morlet_list):\n",
    "\t\tresult[label].append(morlet)\n",
    "\treturn result\n",
    "\n",
    "def ungroup_morlet_by_phoneme(grouped_morlet_list):\n",
    "\t\"\"\"\n",
    "\tPerforms reverse operation by concatenating all groups\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tresult = grouped_morlet_list[0]\n",
    "\tlabels = [ 0 ] * len(grouped_morlet_list[0])\n",
    "\tfor i in range(1, PHONEME_COUNT):\n",
    "\t\tresult = result + grouped_morlet_list[i]\n",
    "\t\tlabels = labels + [ i ] * len(grouped_morlet_list[i])\n",
    "\t\n",
    "\treturn labels, result\n",
    "\n",
    "def save_person_grouped_morlet_list(directory, person, grouped_morlet_list):\n",
    "\t\"\"\"\n",
    "\tSave given morlet data for a person and update total person morlet data count\n",
    "\t\n",
    "\tgrouped_morlet_list is a list containing grouped morlet data for each phoneme ID.\n",
    "\t\n",
    "\tgrouped_morlet_list[0] contains all samples for phoneme 0, e.t.c.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tos.makedirs(directory, exist_ok=True)\n",
    "\t\n",
    "\t# Update total\n",
    "\tset_total_morlet_count(directory, person, sum([ len(gml) for gml in grouped_morlet_list ]))\n",
    "\t\n",
    "\tfor phoneme in range(PHONEME_COUNT):\n",
    "\t\t\n",
    "\t\tgml = grouped_morlet_list[phoneme]\n",
    "\t\t\n",
    "\t\t# Update count\n",
    "\t\tset_morlet_count(directory, person, phoneme, len(gml))\n",
    "\t\t\n",
    "\t\t# Iterate over channels & save morlet data\n",
    "\t\tfor index in range(len(gml)):\n",
    "\t\t\tfor channel in range(TARGET_CHANNELS):\t\n",
    "\t\t\t\tauto_save_morlet(directory, person, phoneme, channel, index, gml[index][channel])\n",
    "\n",
    "def load_person_grouped_morlet_list(directory, person):\n",
    "\t\"\"\"\n",
    "\tLoad data for the gven person\n",
    "\t\"\"\"\n",
    "\t\n",
    "\treturn [\n",
    "\t\t[\n",
    "\t\t\t[\n",
    "\t\t\t\tauto_load_morlet(directory, person, phoneme, channel, index)\n",
    "\t\t\tfor channel in range(TARGET_CHANNELS)\n",
    "\t\t\t]\n",
    "\t\tfor index in range(get_morlet_count(directory, person, phoneme))\n",
    "\t\t]\n",
    "\tfor phoneme in range(PHONEME_COUNT)\n",
    "\t]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data \n",
    "import multiprocessing\n",
    "\n",
    "class MorletDataset(torch.utils.data.Dataset):\n",
    "\t\"\"\"\n",
    "\tDataset class for morlet data\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self, labels, morlets, transform=None, target_transform=None):\n",
    "\t\tself.labels = labels\n",
    "\t\tself.morlets = morlets\n",
    "\t\tself.transform = transform\n",
    "\t\tself.target_transform = target_transform\n",
    "\t\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.labels)\n",
    "\t\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tmorlet, label = self.morlets[idx], self.labels[idx]\n",
    "\t\tif self.transform:\n",
    "\t\t\tmorlet = self.transform(morlet)\n",
    "\t\tif self.target_transform:\n",
    "\t\t\tlabel = self.target_transform(label)\n",
    "\t\treturn morlet, label\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import cv2\n",
    "\n",
    "class NoiseTransform(object):\n",
    "\t\"\"\"\n",
    "\tAdd noise to the sample\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self, noise_scale):\n",
    "\t\tself.noise_scale = noise_scale\n",
    "\n",
    "\tdef __call__(self, sample):\n",
    "\t\tmorlet = sample\n",
    "\t\treturn morlet + np.random.normal(0, self.noise_scale, morlet.shape)\n",
    "\n",
    "# class ShiftTransform(object):\n",
    "# \t\"\"\"\n",
    "# \tRandomly shift morlet spectrogram\n",
    "# \t\"\"\"\n",
    "\n",
    "# \tdef __call__(self, sample):\n",
    "# \t\tmorlet, label = sample\n",
    "# \t\treturn np.roll(morlet, random.randint(0, SECTOR_LENGTH_STEPS)), label\n",
    "\n",
    "class ResizeShiftTransform(object):\n",
    "\t\"\"\"\n",
    "\tPerform rescale of the meorlet and shifting it in a random position\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self, max_scale, max_roll):\n",
    "\t\tself.max_scale = max_scale\n",
    "\t\tself.max_roll = max_roll\n",
    "\t\tself.max_pad = int(SECTOR_LENGTH_STEPS * (max_scale - 1))\n",
    "\n",
    "\tdef __call__(self, sample):\n",
    "\t\tmorlet = sample.transpose(1, 2, 0)\n",
    "\t\trand_pad = random.randint(0, self.max_pad)\n",
    "\t\tresized = cv2.resize(morlet, dsize=(morlet.shape[1] + rand_pad, morlet.shape[0]), interpolation=cv2.INTER_CUBIC).transpose(2, 0, 1)\n",
    "\n",
    "\t\troll = np.roll(resized, -random.randint(0, int(rand_pad * self.max_roll)), 2)\n",
    "\n",
    "\t\tcrop = roll[:,:,0:SECTOR_LENGTH_STEPS]\n",
    "\t\t\n",
    "\t\treturn crop\n",
    "\n",
    "class FlipAlongTime(object):\n",
    "\t\"\"\"\n",
    "\tFlip data along time axis\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __call__(self, sample):\n",
    "\t\treturn np.flip(sample, axis=2) if random.randint(0, 1) else sample\n",
    "\n",
    "class ToTensor(object):\n",
    "\t\"\"\"\n",
    "\tConvert ndarray to tensor\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __call__(self, sample):\n",
    "\t\tmorlet = sample\n",
    "\t\t\n",
    "\t\treturn torch.from_numpy(morlet.copy())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNetwork(torch.nn.Module):\n",
    "\t\"\"\"\n",
    "\tBase blass for morlet classification network.\n",
    "\t\n",
    "\tconv_layers - list of dicts defining each convolutional layer\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, conv_layers, dense_layers, num_classes, use_conv_sigmoid=False, use_dense_sigmoid=False, print_log=True):\n",
    "\t\tsuper().__init__()\n",
    "\t\t\n",
    "\t\t# Conv layers\n",
    "\t\tself.conv = []\n",
    "\t\t# MaxPool layers\n",
    "\t\tself.pool = []\n",
    "\t\t# Dropout for layers\n",
    "\t\tself.conv_dropout = []\n",
    "\t\t# Fully connected layers\n",
    "\t\tself.fc = []\n",
    "\t\t# Dropout for layers\n",
    "\t\tself.fc_dropout = []\n",
    "\t\t\n",
    "\t\t# Use sigmoid or relu\n",
    "\t\tself.use_conv_sigmoid = use_conv_sigmoid\n",
    "\t\tself.use_dense_sigmoid = use_dense_sigmoid\n",
    "\t\t\n",
    "\t\t# Save useful parameters\n",
    "\t\tself.conv_layers = conv_layers\n",
    "\t\tself.dense_layers = dense_layers\n",
    "\t\tself.num_classes = num_classes\n",
    "\t\t\n",
    "\t\t# Add all conv layers\n",
    "\t\t# Input conv layer 'in' parameter is ignored and equals to TARGET_CHANNELS x MORLET_FREQ_STEPS x SECTOR_LENGTH_STEPS\n",
    "\t\t# Kernel is tuple\n",
    "\t\t# pool is tuple\n",
    "\t\tlast_conv_out = TARGET_CHANNELS\n",
    "\t\tresult_dimension = [ TARGET_CHANNELS, MORLET_FREQ_STEPS, SECTOR_LENGTH_STEPS ]\n",
    "\t\tfor i, layer in enumerate(conv_layers):\n",
    "\t\t\tself.conv.append(torch.nn.Conv2d(last_conv_out, layer['out'], layer['kernel']))\n",
    "\t\t\tself.__setattr__(f'conv{i}', self.conv[-1])\n",
    "\t\t\tlast_conv_out = layer['out']\n",
    "\t\t\t\n",
    "\t\t\tself.pool.append(torch.nn.MaxPool2d(layer['pool']))\n",
    "\t\t\tself.__setattr__(f'pool{i}', self.pool[-1])\n",
    "\t\t\t\n",
    "\t\t\tself.conv_dropout.append(torch.nn.Dropout(layer['dropout']))\n",
    "\t\t\tself.__setattr__(f'conv_dropout{i}', self.conv_dropout[-1])\n",
    "\t\t\t\n",
    "\t\t\t# Recalculate dimensions\n",
    "\t\t\tresult_dimension[0] = layer['out']\n",
    "\t\t\tresult_dimension[1] = result_dimension[1] - layer['kernel'][0] + 1\n",
    "\t\t\tresult_dimension[2] = result_dimension[2] - layer['kernel'][1] + 1\n",
    "\t\t\tresult_dimension[1] = result_dimension[1] // layer['pool'][0]\n",
    "\t\t\tresult_dimension[2] = result_dimension[2] // layer['pool'][1]\n",
    "\t\t\n",
    "\t\tif print_log:\n",
    "\t\t\tprint(f'conv-dense dimension:', result_dimension)\n",
    "\t\t\n",
    "\t\t# Add fully connected\n",
    "\t\t# dense_layers contain only layers sizes between input and output of dense net\n",
    "\t\t# Input of the dense has dimensions of the last conv layer\n",
    "\t\t# Outputs of the dense has dimensions of the num_classes\n",
    "\t\tlast_fc_out = result_dimension[0] * result_dimension[1] * result_dimension[2]\n",
    "\t\tfor i, layer in enumerate(dense_layers):\n",
    "\t\t\tself.fc.append(torch.nn.Linear(last_fc_out, layer['count']))\n",
    "\t\t\tself.__setattr__(f'fc{i}', self.fc[-1])\n",
    "\t\t\tlast_fc_out = layer['count']\n",
    "\t\t\t\n",
    "\t\t\tself.fc_dropout.append(torch.nn.Dropout(layer['dropout']))\n",
    "\t\t\tself.__setattr__(f'fc_dropout{i}', self.fc_dropout[-1])\n",
    "\t\t\n",
    "\t\t# Append last fc layer\n",
    "\t\tself.fc.append(torch.nn.Linear(last_fc_out, num_classes))\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\t\n",
    "\t\t# Apply conv\n",
    "\t\tfor conv, pool, drop in zip(self.conv, self.pool, self.conv_dropout):\n",
    "\t\t\tif self.use_conv_sigmoid:\n",
    "\t\t\t\tx = drop(pool(torch.sigmoid(conv(x))))\n",
    "\t\t\telse:\n",
    "\t\t\t\tx = drop(pool(torch.relu(conv(x))))\n",
    "\t\t\n",
    "\t\t# Flatten\n",
    "\t\tx = torch.flatten(x, 1)\n",
    "\t\t\n",
    "\t\t# Dense\n",
    "\t\tfor fc, drop in zip(self.fc[:-1], self.fc_dropout):\n",
    "\t\t\tif self.use_dense_sigmoid:\n",
    "\t\t\t\tx = drop(torch.sigmoid(fc(x)))\n",
    "\t\t\telse:\n",
    "\t\t\t\tx = drop(torch.relu(fc(x)))\n",
    "\t\t\n",
    "\t\tx = self.fc[-1](x)\n",
    "\t\t\n",
    "\t\treturn x\n",
    "\t\n",
    "\tdef save_model(self, filename):\n",
    "\t\tconfig = {\n",
    "\t\t\t'conv_layers': self.conv_layers,\n",
    "\t\t\t'dense_layers': self.dense_layers,\n",
    "\t\t\t'num_classes': self.num_classes,\n",
    "\t\t\t'use_conv_sigmoid': self.use_conv_sigmoid,\n",
    "\t\t\t'use_dense_sigmoid': self.use_dense_sigmoid\n",
    "\t\t}\n",
    "\t\t\n",
    "\t\t# Save config\n",
    "\t\twith open(f'{filename}-config.json', 'w') as f:\n",
    "\t\t\tjson.dump(config, f)\n",
    "\t\t\n",
    "\t\t# Save all parameters\n",
    "\t\tfor i, conv in enumerate(self.conv):\n",
    "\t\t\tstate = conv.state_dict()\n",
    "\t\t\t\n",
    "\t\t\tfor key in state.keys():\n",
    "\t\t\t\ttorch.save(state[key], f'{filename}-conv{i}-{key}.tensor')\n",
    "\t\t\n",
    "\t\t# Save all parameters\n",
    "\t\tfor i, fc in enumerate(self.fc):\n",
    "\t\t\tstate = fc.state_dict()\n",
    "\t\t\t\n",
    "\t\t\tfor key in state.keys():\n",
    "\t\t\t\ttorch.save(state[key], f'{filename}-fc{i}-{key}.tensor')\n",
    "\t\n",
    "\tdef load_model(filename):\n",
    "\t\t# Save config\n",
    "\t\twith open(f'{filename}-config.json', 'r') as f:\n",
    "\t\t\tconfig = json.load(f)\n",
    "\t\t\n",
    "\t\t# Create model\n",
    "\t\tmodel = MNetwork(config['conv_layers'], config['dense_layers'], config['num_classes'], config['use_conv_sigmoid'], config['use_dense_sigmoid'])\n",
    "\t\t\n",
    "\t\t# Save all parameters\n",
    "\t\tfor i, conv in enumerate(model.conv):\n",
    "\t\t\tstate = conv.state_dict()\n",
    "\t\t\t\n",
    "\t\t\tfor key in state.keys():\n",
    "\t\t\t\tstate[key] = torch.load(f'{filename}-conv{i}-{key}.tensor')\n",
    "\t\t\t\n",
    "\t\t\tconv.load_state_dict(state)\n",
    "\t\t\n",
    "\t\t# Save all parameters\n",
    "\t\tfor i, fc in enumerate(model.fc):\n",
    "\t\t\tstate = fc.state_dict()\n",
    "\t\t\t\n",
    "\t\t\tfor key in state.keys():\n",
    "\t\t\t\tstate[key] = torch.load(f'{filename}-fc{i}-{key}.tensor')\n",
    "\t\t\t\n",
    "\t\t\tfc.load_state_dict(state)\n",
    "\t\t\n",
    "\t\treturn model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_match_on_dataset(model: MNetwork, test_dataset):\n",
    "\tcount = 0\n",
    "\twith torch.no_grad():\n",
    "\t\tmodel.eval()\n",
    "\t\tfor morlet, label in test_dataset:\n",
    "\t\t\toutput = model(morlet[None, ...].float())\n",
    "\t\t\tif output.shape[-1] == 1:\n",
    "\t\t\t\tcount += ((output > 0.5).int() == label).sum().item()\n",
    "\t\t\telse:\n",
    "\t\t\t\tpredicted = torch.argmax(output, 1)\n",
    "\t\t\t\tcount += (predicted == label).sum().item()\n",
    "\t\n",
    "\treturn count / len(test_dataset)\n",
    "\n",
    "def calculate_fail_on_dataset(model: MNetwork, test_dataset):\n",
    "\tcount = 0\n",
    "\twith torch.no_grad():\n",
    "\t\tmodel.eval()\n",
    "\t\tfor morlet, label in test_dataset:\n",
    "\t\t\toutput = model(morlet[None, ...].float())\n",
    "\t\t\tif output.shape[-1] == 1:\n",
    "\t\t\t\tcount += ((output > 0.5).int() != label).sum().item()\n",
    "\t\t\telse:\n",
    "\t\t\t\tpredicted = torch.argmax(output, 1)\n",
    "\t\t\t\tcount += (predicted != label).sum().item()\n",
    "\t\n",
    "\treturn count / len(test_dataset)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: MNetwork, train_loader, optimizer, scheduler, criterion, train_epochs, print_log=False, print_iters=100, print_first_iter=True, print_last_iter=True):\n",
    "\t\"\"\"\n",
    "\tTrain model for given amount of epochs\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tmodel.train()\n",
    "\tfor epoch in range(train_epochs):\n",
    "\t\tfor iter, (morlets, labels) in enumerate(train_loader):\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\toutputs = model(morlets.float())\n",
    "\t\t\t\n",
    "\t\t\tif outputs.shape[-1] == 1:\n",
    "\t\t\t\toutputs = torch.sigmoid(outputs.flatten())\n",
    "\t\t\t\tloss = criterion(outputs, labels.float())\n",
    "\t\t\telse:\n",
    "\t\t\t\tloss = criterion(outputs, labels.long())\n",
    "\t\t\t\t\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\t\n",
    "\t\t\tif print_log:\n",
    "\t\t\t\tif (iter % print_iters == 0):\n",
    "\t\t\t\t\tif iter == 0 and print_first_iter:\n",
    "\t\t\t\t\t\tprint(f'epoch = {epoch}, iter = {iter}, loss = {loss.item()}')\n",
    "\t\t\t\t\telif iter > 0:\n",
    "\t\t\t\t\t\tprint(f'epoch = {epoch}, iter = {iter}, loss = {loss.item()}')\n",
    "\t\t\t\telif print_last_iter and iter == len(train_loader) - 1:\n",
    "\t\t\t\t\tprint(f'epoch = {epoch}, iter = {iter}, loss = {loss.item()}')\n",
    "\t\t\n",
    "\t\tif scheduler is not None:\n",
    "\t\t\tscheduler.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset utilities for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(visual = None, person = None, sector_length_steps = None, morlet_freq_steps = None):\n",
    "\t\"\"\"\n",
    "\tLoad data with given options.\n",
    "\tIf option is set to None, data  for all values of this optio is loaded.\n",
    "\t\n",
    "\tReturns labels, morlets in ungrouped mode\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tlabels, morlets = [], []\n",
    "\t\n",
    "\tperson_list = [ person ]\n",
    "\tvisual_list = [ visual ]\n",
    "\t\n",
    "\tif person is None:\n",
    "\t\tperson_list = list(range(len(INPUT_EDF_LIST)))\n",
    "\t\n",
    "\tif visual is None:\n",
    "\t\tvisual_list = [ False, True ]\n",
    "\t\n",
    "\tsector_length_steps = sector_length_steps if sector_length_steps is not None else SECTOR_LENGTH_STEPS\n",
    "\tmorlet_freq_steps = morlet_freq_steps if morlet_freq_steps is not None else MORLET_FREQ_STEPS\n",
    "\t\n",
    "\tfor visual in visual_list:\n",
    "\t\tfor person in person_list:\n",
    "\t\t\t# Subdirectory matching oble audial or visual\n",
    "\t\t\tedf_subdir = VISUAL_SUBPATH if visual else AUDIAL_SUBPATH\n",
    "\t\t\t# Full directory path to morlet files\n",
    "\t\t\tmorlet_dir = f'{MORLET_ORIGINAL_SAVE_DIR}/width-{sector_length_steps}_height-{morlet_freq_steps}/{edf_subdir}'\n",
    "\n",
    "\t\t\t# Load\n",
    "\t\t\tloaded_data = load_person_grouped_morlet_list(morlet_dir, person=person)\n",
    "\t\t\t\n",
    "\t\t\tloaded_labels, loaded_morlets = ungroup_morlet_by_phoneme(loaded_data)\n",
    "\t\t\t\n",
    "\t\t\tlabels += loaded_labels\n",
    "\t\t\tmorlets += loaded_morlets\n",
    "\t\n",
    "\treturn np.array(labels), np.array(morlets)\n",
    "\n",
    "def select_phonemes(labels, morlets, phoneme_pair = None):\n",
    "\t\"\"\"\n",
    "\tSelect given pair of phonemes from given data\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tif phoneme_pair is None:\n",
    "\t\treturn labels, morlets\n",
    "\t\n",
    "\tlabels, morlets = labels.copy(), morlets.copy()\n",
    "\t\n",
    "\t# Sumselect required classes\n",
    "\tif phoneme_pair is not None:\n",
    "\t\tcond = np.isin(labels, phoneme_pair)\n",
    "\t\tmorlets = morlets[cond]\n",
    "\t\tlabels = labels[cond]\n",
    "\t\t\n",
    "\t\tfor i in range(len(labels)):\n",
    "\t\t\tif labels[i] == phoneme_pair[0]:\n",
    "\t\t\t\tlabels[i] = 0\n",
    "\t\t\telse:\n",
    "\t\t\t\tlabels[i] = 1\n",
    "\t\n",
    "\treturn labels, morlets\n",
    "\n",
    "def normalize_morlets(morlets):\n",
    "\treturn morlets / (morlets.max() - morlets.min())\n",
    "\n",
    "def train_test_split(labels, morlets, test_size):\n",
    "\tindices = np.arange(len(morlets))\n",
    "\tnp.random.shuffle(indices)\n",
    "\tlabels, morlets = labels[indices], morlets[indices]\n",
    "\ttrain_count = int(len(morlets) * (1.0 - test_size))\n",
    "\t\n",
    "\ttrain_labels, train_morlets = labels[0:train_count], morlets[0:train_count]\n",
    "\ttest_labels, test_morlets = labels[train_count:-1], morlets[train_count:-1]\n",
    "\treturn train_labels, train_morlets, test_labels, test_morlets\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > Sample usage demos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load EDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_DEMO:\n",
    "\tedf = open_audial_edf(f'{INPUT_EDF_LIST[0]}.edf')\n",
    "\n",
    "\t# New\n",
    "\tsectors, invalid_sectors, labels = extract_strict_sectors(edf, SECTOR_LENGTH)\n",
    "\tprint_sectors_summary(edf, sectors, invalid_sectors)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_DEMO:\n",
    "\tplot_labels(edf, invalid_sectors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_DEMO:\n",
    "\tplot_sectors(sectors, invalid_sectors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_DEMO:\n",
    "\tchannels_data = subselect_channels(edf)\n",
    "\t_, lengths, durations, splitted = split_sectors(edf, channels_data, sectors)\n",
    "\tt, freq, morlet = morlet_wavelet_pass(splitted)\n",
    "\tmorlet = transpose_morlet_channel_data(morlet)\n",
    "\tmorlet = abs_morlet_data(morlet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview channel samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_DEMO:\n",
    "\tindex = 0\n",
    "\n",
    "\tplt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "\tplt.rcParams[\"font.size\"] = 14\n",
    "\tfig, axs = plt.subplots(2, 2)\n",
    "\n",
    "\tfor channel in range(4):\n",
    "\t\tvalue = np.abs(morlet[index][channel])\n",
    "\t\tprint(t.shape, freq.shape, value.shape)\n",
    "\t\t\n",
    "\t\taxs[channel % 2][channel // 2].pcolormesh(t, freq, value, cmap='viridis', shading='gouraud')\n",
    "\t\n",
    "\tplt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_DEMO:\n",
    "\tnormalized_labels = normalize_labels(labels)\n",
    "\tgrouped_morlet_list = group_morlet_by_phoneme(normalized_labels, morlet)\n",
    "\t\n",
    "\tprint('phonemes:   ', 'len(grouped_morlet_list)', len(grouped_morlet_list))\n",
    "\tprint('samples ph0:', 'len(grouped_morlet_list[0])', len(grouped_morlet_list[0]))\n",
    "\tprint(' total:     ', sum([ len(s) for s in grouped_morlet_list ]))\n",
    "\tprint('channels:   ', 'len(grouped_morlet_list[0][0])', len(grouped_morlet_list[0][0]))\n",
    "\tprint('frequencies:', 'len(grouped_morlet_list[0][0][0])', len(grouped_morlet_list[0][0][0]))\n",
    "\tprint('ticks       ', 'len(grouped_morlet_list[0][0][0][0])', len(grouped_morlet_list[0][0][0][0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example save & load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_DEMO:\n",
    "\tsave_person_grouped_morlet_list(MORLET_ORIGINAL_SAVE_DIR, 0, grouped_morlet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_DEMO:\n",
    "\tloaded_data = load_person_grouped_morlet_list(MORLET_ORIGINAL_SAVE_DIR, person=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_DEMO:\n",
    "\tprint('phonemes:   ', 'len(grouped_morlet_list)', len(loaded_data))\n",
    "\tprint('samples ph0:', 'len(grouped_morlet_list[0])', len(loaded_data[0]))\n",
    "\tprint(' total:     ', sum([ len(s) for s in loaded_data ]))\n",
    "\tprint('channels:   ', 'len(grouped_morlet_list[0][0])', len(loaded_data[0][0]))\n",
    "\tprint('frequencies:', 'len(grouped_morlet_list[0][0][0])', len(loaded_data[0][0][0]))\n",
    "\tprint('ticks       ', 'len(grouped_morlet_list[0][0][0][0])', len(loaded_data[0][0][0][0]))\n",
    "\t\n",
    "\tlabels, ungrouped_loaded_data = ungroup_morlet_by_phoneme(loaded_data)\n",
    "\tungrouped_loaded_data = np.asarray(ungrouped_loaded_data)\n",
    "\tungrouped_loaded_data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample network init + save & load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_DEMO:\n",
    "\tconv_layers = [\n",
    "\t\t{\n",
    "\t\t\t'out': 8,\n",
    "\t\t\t'kernel': (5, 10),\n",
    "\t\t\t'pool': (2, 2)\n",
    "\t\t}\n",
    "\t]\n",
    "\n",
    "\tdense_layers = [\n",
    "\t\t\n",
    "\t]\n",
    "\n",
    "\tnum_classes = 2\n",
    "\n",
    "\tmodel = MNetwork(conv_layers, dense_layers, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_DEMO:\n",
    "\tmodel.save_model('temp/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_DEMO:\n",
    "\tmodel = MNetwork.load_model('temp/model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > Perform wavelet pass over all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONVERT_MORLETS:\n",
    "\tfor visual in [ False, True ]:\n",
    "\t\t# Subdirectory matching oble audial or visual\n",
    "\t\tedf_subdir = VISUAL_SUBPATH if visual else AUDIAL_SUBPATH\n",
    "\t\t# Full directory path to input edf\n",
    "\t\tedf_dir = f'{CLEARED_PATH}/{edf_subdir}'\n",
    "\t\t# Full directory path to morlet files\n",
    "\t\tmorlet_dir = f'{MORLET_ORIGINAL_SAVE_DIR}/width-{SECTOR_LENGTH_STEPS}_height-{MORLET_FREQ_STEPS}/{edf_subdir}'\n",
    "\t\t\n",
    "\t\tprint()\n",
    "\t\tprint(f'Preprocessing data in {edf_dir}')\n",
    "\t\tprint(f'Wriging morlets to {morlet_dir}')\n",
    "\t\t\n",
    "\t\tfor person, edf_file in enumerate(INPUT_EDF_LIST):\n",
    "\t\t\tprint()\n",
    "\t\t\tprint(f'Processing {edf_file}')\n",
    "\t\t\t\n",
    "\t\t\t# Open\n",
    "\t\t\tif visual:\n",
    "\t\t\t\tedf = open_visual_edf(f'{edf_file}.edf')\n",
    "\t\t\telse:\n",
    "\t\t\t\tedf = open_audial_edf(f'{edf_file}.edf')\n",
    "\t\t\t\n",
    "\t\t\t# Select segments\n",
    "\t\t\tsectors, invalid_sectors, labels = extract_strict_sectors(edf, SECTOR_LENGTH)\n",
    "\t\t\tprint_sectors_summary(edf, sectors, invalid_sectors)\n",
    "\t\t\t\n",
    "\t\t\t# Morlet transform\n",
    "\t\t\tprint(f'Applying morlet transform for {edf_file}')\n",
    "\t\t\tchannels_data = subselect_channels(edf)\n",
    "\t\t\t_, lengths, durations, splitted = split_sectors(edf, channels_data, sectors)\n",
    "\t\t\tt, freq, morlet = morlet_wavelet_pass(splitted)\n",
    "\t\t\tmorlet = transpose_morlet_channel_data(morlet)\n",
    "\t\t\tmorlet = abs_morlet_data(morlet)\n",
    "\t\t\t\n",
    "\t\t\t# print(morlet[0].shape, freq.shape, t.shape)\n",
    "\t\t\t# break\n",
    "\t\t\t\n",
    "\t\t\tprint('t.shape', t.shape)\n",
    "\t\t\tprint('freq.shape', freq.shape)\n",
    "\t\t\tprint('morlet.shape', morlet.shape)\n",
    "\t\t\t\n",
    "\t\t\t# Save data\n",
    "\t\t\tprint(f'Saving morlet data for {edf_file}')\n",
    "\t\t\tnormalized_labels = normalize_labels(labels)\n",
    "\t\t\tgrouped_morlet_list = group_morlet_by_phoneme(normalized_labels, morlet)\n",
    "\t\t\tsave_person_grouped_morlet_list(morlet_dir, person, grouped_morlet_list)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > Train for different tasks with diferent configurations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy data preload for large datasets and multiperson classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable presloading dataset for ALL persons as one\n",
    "PRELOAD_ALL_DATASET = False\n",
    "\n",
    "if PRELOAD_ALL_DATASET:\n",
    "\tbase_labels, base_morlets = load_dataset(None, None)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test running code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import multiprocessing\n",
    "\n",
    "def run_test():\n",
    "\n",
    "\tif SAVE_CONFIG:\n",
    "\t\twith open(test_config.test_config_json, 'w') as f:\n",
    "\t\t\tjson.dump(test_config, f)\n",
    "\n",
    "\tprint('binary:', test_config.binary)\n",
    "\n",
    "\t# What data to use during train\n",
    "\tif test_config.all_person_data:\n",
    "\t\t# List of person IDS to fetch data from\n",
    "\t\tpersons_list = [ None ]\n",
    "\t\tprint('Combine all person data')\n",
    "\t\t\n",
    "\telse:\n",
    "\t\tif test_config.person is None:\n",
    "\t\t\tpersons_list = list(range(len(INPUT_EDF_LIST)))\n",
    "\t\t\tprint('Train on each person separately')\n",
    "\t\telse:\n",
    "\t\t\tpersons_list = [ test_config.person ]\n",
    "\t\t\tprint('Train on single person:', test_config.person)\n",
    "\n",
    "\t# What phoneme combinations to use\n",
    "\tif test_config.binary:\n",
    "\t\tif test_config.phoneme_classes is not None:\n",
    "\t\t\t# Binary classifier for given class numbers\n",
    "\t\t\tphoneme_list = [ test_config.phoneme_classes ]\n",
    "\t\t\tprint('Train on single phoneme pair:', test_config.phoneme_classes)\n",
    "\t\telse:\n",
    "\t\t\t# Binary classifier for every pair of classes\n",
    "\t\t\tphoneme_list = []\n",
    "\t\t\tfor i in range(PHONEME_COUNT):\n",
    "\t\t\t\tfor j in range(PHONEME_COUNT):\n",
    "\t\t\t\t\tif i > j:\n",
    "\t\t\t\t\t\tphoneme_list.append((i, j))\n",
    "\t\t\tprint('Train on each phoneme pair')\n",
    "\telse:\n",
    "\t\t# Select data for given list of classes\n",
    "\t\tphoneme_list = [ None ]\n",
    "\t\t\n",
    "\t\tif test_config.phoneme_classes is None:\n",
    "\t\t\tprint('Train on all phoneme classes')\n",
    "\t\telse:\n",
    "\t\t\tprint('Train on selected phoneme classes:', test_config.phoneme_classes)\n",
    "\t\n",
    "\tfor person in persons_list:\n",
    "\t\tglobal base_labels\n",
    "\t\tglobal base_morlets\n",
    "\t\t\n",
    "\t\tif RELOAD_DATASET:\n",
    "\t\t\tbase_labels, base_morlets = load_dataset(test_config.visual, person)\n",
    "\t\t\tprint('labels:', set(base_labels))\n",
    "\t\telse:\n",
    "\t\t\tprint('-----------------------------')\n",
    "\t\t\tprint('WARNING: Dataset not reloaded')\n",
    "\t\t\tprint('-----------------------------')\n",
    "\t\t\n",
    "\t\tfor phoneme_pair in phoneme_list:\n",
    "\t\t\t\n",
    "\t\t\tfor seed in range(test_config.seed_steps):\n",
    "\t\t\t\tseed += test_config.seed\n",
    "\t\t\t\t\n",
    "\t\t\t\tprint('seed', seed)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\t\t\t\t# L O A D   D A T A\n",
    "\t\t\t\t# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\t\t\t\t\n",
    "\t\t\t\tif True:\n",
    "\t\t\t\t\tprint()\n",
    "\t\t\t\t\tprint()\n",
    "\t\t\t\t\tif test_config.binary:\n",
    "\t\t\t\t\t\tprint('person =', person, 'phoneme1 =', phoneme_pair[0], 'phoneme2 =', phoneme_pair[1])\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tprint('person =', person, 'classes =', test_config.phoneme_classes)\n",
    "\t\t\t\t\n",
    "\t\t\t\t\t# Phoneme classes to select from data\n",
    "\t\t\t\t\tif test_config.binary:\n",
    "\t\t\t\t\t\tphoneme_classes = phoneme_pair\n",
    "\t\t\t\t\t\tnum_classes = 1\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tif test_config.phoneme_classes is None:\n",
    "\t\t\t\t\t\t\tphoneme_classes = None # list(range(PHONEME_COUNT))\n",
    "\t\t\t\t\t\t\tnum_classes = PHONEME_COUNT\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tphoneme_classes = test_config.phoneme_classes\n",
    "\t\t\t\t\t\t\tnum_classes =  len(test_config.phoneme_classes)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Load dataset & select phonemes\n",
    "\t\t\t\t\tlabels, morlets = select_phonemes(base_labels, base_morlets, phoneme_classes)\n",
    "\t\t\t\t\tmorlets = normalize_morlets(morlets)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tprint('labels stats:')\n",
    "\t\t\t\t\tstats = np.unique(labels, return_counts=True)\n",
    "\t\t\t\t\tprint('\\n'.join([ f'{v[0]}: {v[1]}' for v in zip(stats[0], stats[1])]))\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Set seed\n",
    "\t\t\t\t\tnp.random.seed(seed)\n",
    "\t\t\t\t\ttorch.manual_seed(seed)\n",
    "\n",
    "\t\t\t\t\t# Split\n",
    "\t\t\t\t\ttrain_labels, train_morlets, test_labels, test_morlets = train_test_split(labels, morlets, test_config.test_size)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tprint('train labels stats:')\n",
    "\t\t\t\t\tstats = np.unique(train_labels, return_counts=True)\n",
    "\t\t\t\t\tprint('\\n'.join([ f'{v[0]}: {v[1]}' for v in zip(stats[0], stats[1])]))\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tprint('test labels stats:')\n",
    "\t\t\t\t\tstats = np.unique(test_labels, return_counts=True)\n",
    "\t\t\t\t\tprint('\\n'.join([ f'{v[0]}: {v[1]}' for v in zip(stats[0], stats[1])]))\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tprint('Train count:', len(train_labels))\n",
    "\t\t\t\t\tprint('Test count: ', len(test_labels))\n",
    "\n",
    "\t\t\t\t\t# Transform\n",
    "\t\t\t\t\ttrain_transform = torchvision.transforms.Compose([\n",
    "\t\t\t\t\t\tResizeShiftTransform(test_config.shift_transform_scale, test_config.shift_transform_roll),\n",
    "\t\t\t\t\t\tNoiseTransform(test_config.noise_transform_scale),\n",
    "\t\t\t\t\t\tFlipAlongTime(),\n",
    "\t\t\t\t\t\tToTensor()\n",
    "\t\t\t\t\t])\n",
    "\n",
    "\t\t\t\t\ttest_transform = torchvision.transforms.Compose([\n",
    "\t\t\t\t\t\tToTensor()\n",
    "\t\t\t\t\t])\n",
    "\n",
    "\t\t\t\t\t# Dataset\n",
    "\t\t\t\t\ttrain, test = MorletDataset(train_labels, train_morlets, train_transform), MorletDataset(test_labels, test_morlets, test_transform)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Overall dataset\n",
    "\t\t\t\t\toverall = MorletDataset(labels, morlets, test_transform)\t\t\t\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# DataLoader for train\n",
    "\t\t\t\t\ttrain_loader = torch.utils.data.DataLoader(train, batch_size=test_config.batch_size, shuffle=True) # , num_workers=test_config.num_workers)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Set seed\n",
    "\t\t\t\t\tnp.random.seed(seed)\n",
    "\t\t\t\t\ttorch.manual_seed(seed)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\t\t\t\t# T R A I N _ M O D E L\n",
    "\t\t\t\t# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\t\t\t\t\n",
    "\t\t\t\tif True:\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tprint('num_classes', num_classes)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Create model\n",
    "\t\t\t\t\tmodel = MNetwork(\n",
    "\t\t\t\t\t\ttest_config.conv_layers, \n",
    "\t\t\t\t\t\ttest_config.dense_layers, \n",
    "\t\t\t\t\t\tnum_classes, \n",
    "\t\t\t\t\t\tuse_conv_sigmoid=test_config.use_conv_sigmoid, \n",
    "\t\t\t\t\t\tuse_dense_sigmoid=test_config.use_dense_sigmoid\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\t\tprint('Network:', 'conv_layers', test_config.conv_layers, 'dense_layers', test_config.dense_layers, 'num_classes', num_classes, 'use_conv_sigmoid', test_config.use_conv_sigmoid, 'use_dense_sigmoid', test_config.use_dense_sigmoid)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tprint(f'Train size:    {len(train)}')\n",
    "\t\t\t\t\tprint(f'Test size:     {len(test)}')\n",
    "\t\t\t\t\tinit_match = 0\n",
    "\t\t\t\t\tprint(f'Initial match: {round(init_match := calculate_match_on_dataset(model, test) * 100, 2)}%')\n",
    "\t\t\t\t\tinit_overall_match = 0\n",
    "\t\t\t\t\tprint(f'Initial overall match: {round(init_overall_match := calculate_match_on_dataset(model, overall) * 100, 2)}%')\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Training requirements\n",
    "\t\t\t\t\tif num_classes > 1:\n",
    "\t\t\t\t\t\tcriterion = torch.nn.CrossEntropyLoss()\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tcriterion = torch.nn.BCELoss()\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "\t\t\t\t\toptimizer = torch.optim.SGD(model.parameters(), lr=test_config.lr_start, momentum=0.9)\n",
    "\t\t\t\t\tscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=test_config.lr_step_size, gamma=0.1)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Train 5 epochs\n",
    "\t\t\t\t\ttrain_model(model, train_loader, optimizer, scheduler, criterion, test_config.epochs, print_log=True, print_iters=100, print_last_iter=False)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Save global checkpoint\n",
    "\t\t\t\t\tif GLOBAL_LAST_CHECKPOINT:\n",
    "\t\t\t\t\t\tglobal checkpoint\n",
    "\t\t\t\t\t\tcheckpoint = model\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Evaluate\n",
    "\t\t\t\t\tresult_match = 0\n",
    "\t\t\t\t\tprint(f'After train match: {round(result_match := calculate_match_on_dataset(model, test) * 100, 2)}%')\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Evaluate\n",
    "\t\t\t\t\tresult_overall_match = 0\n",
    "\t\t\t\t\tprint(f'Overall train match: {round(result_overall_match := calculate_match_on_dataset(model, overall) * 100, 2)}%')\n",
    "\t\t\t\t\n",
    "\t\t\t\t# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\t\t\t\t# S A V E   M O D E L\n",
    "\t\t\t\t# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\t\t\t\t\n",
    "\t\t\t\tif SAVE_CHECKPOINTS:\n",
    "\t\t\t\t\tif test_config.binary:\n",
    "\t\t\t\t\t\tcheckpoint_name = f'checkpoint-{test_config.checkpoint_prefix}-{test_config.visual}-{person}-binary-ph_{phoneme_pair[0]}-ph_{phoneme_pair[1]}-ep_{test_config.epochs}-seed_{seed}'\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tcheckpoint_name = f'checkpoint-{test_config.checkpoint_prefix}-{test_config.visual}-{person}-multiclass-phs_{\"_\".join(test_config.phoneme_classes or [ \"all\" ])}-ep_{test_config.epochs}-seed_{seed}'\n",
    "\t\t\t\t\tmodel.save_model(f'checkpoints/{checkpoint_name}')\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tcheckpoint_info = {}\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\twith open(test_config.test_json, 'r') as f:\n",
    "\t\t\t\t\t\t\tcheckpoint_info = json.load(f)\n",
    "\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\tpass\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tcheckpoint_info[checkpoint_name] = {\n",
    "\t\t\t\t\t\t'init_match': init_match,\n",
    "\t\t\t\t\t\t'result_match': result_match,\n",
    "\t\t\t\t\t\t'init_overall_match': init_overall_match,\n",
    "\t\t\t\t\t\t'result_overall_match': result_overall_match\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\twith open(test_config.test_json, 'w') as f:\n",
    "\t\t\t\t\t\t\tjson.dump(checkpoint_info, f)\n",
    "\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\tprint('Saving error')\n",
    "\t\t\t\t\t\tprint(checkpoint_info)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary single person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable / disable config\n",
    "BINARY_SINGLE = False\n",
    "\n",
    "if BINARY_SINGLE:\n",
    "\t\n",
    "\ttest_config = dotdict()\n",
    "\n",
    "\t# Data selection\n",
    "\t# Whiat data to use (visual, audial, all together)\n",
    "\ttest_config.visual          = None\n",
    "\n",
    "\t# Set to True to enable binary classification mode\n",
    "\ttest_config.binary          = True\n",
    "\n",
    "\tif test_config.binary:\n",
    "\t\t\n",
    "\t\t# - - - B I N A R Y   O P T I O N S - - - \n",
    "\n",
    "\t\t# What person to use to train on\n",
    "\t\t# Set to None to train on each person\n",
    "\t\t# Set to number to train on single person\n",
    "\t\ttest_config.person          = None\n",
    "\n",
    "\t\t# Set to True to combine data from all persons into single dataset\n",
    "\t\ttest_config.all_person_data = False\n",
    "\n",
    "\t\t# What phonemes to use during train\n",
    "\t\t# Set to None to train on each pair combination\n",
    "\t\t# Set to pair to train on this data pair\n",
    "\t\ttest_config.phoneme_classes = None\n",
    "\n",
    "\telse:\n",
    "\t\t\n",
    "\t\t# - - - M U L T I C L A S S   O P T I O N S - - - \n",
    "\n",
    "\t\t# List of persons to train on\n",
    "\t\t# Set to None to train on each person\n",
    "\t\t# Set to number to train on single person\n",
    "\t\ttest_config.person          = 1\n",
    "\n",
    "\t\t# Set to True to combine data from all persons into single dataset\n",
    "\t\ttest_config.all_person_data = True\n",
    "\n",
    "\t\t# Phoneme classes to use during train\n",
    "\t\t# Set to None to use all phonemes\n",
    "\t\ttest_config.phoneme_classes = None\n",
    "\n",
    "\t# Train properties\n",
    "\ttest_config.test_size         = 0.2\n",
    "\ttest_config.batch_size        = 4\n",
    "\ttest_config.epochs            = 50\n",
    "\ttest_config.use_conv_sigmoid  = False\n",
    "\ttest_config.use_dense_sigmoid = False # test_config.binary\n",
    "\n",
    "\t# Train autoconfig\n",
    "\ttest_config.lr_step_size     = 20\n",
    "\ttest_config.lr_start         = 0.01\n",
    "\n",
    "\t# Transform properties\n",
    "\ttest_config.shift_transform_scale = 1.1\n",
    "\ttest_config.shift_transform_roll  = 1.0\n",
    "\ttest_config.noise_transform_scale = 0.002\n",
    "\n",
    "\ttest_config.seed       = 43\n",
    "\ttest_config.seed_steps = 1\n",
    "\ttest_config.num_workers = multiprocessing.cpu_count() // 4\n",
    "\n",
    "\t# Output properties\n",
    "\ttimestamp                     = round(time.time() * 1000)\n",
    "\ttest_config.checkpoint_prefix = f'autorun-{timestamp}'\n",
    "\ttest_config.test_json         = f'checkpoints/{test_config.checkpoint_prefix}.json'\n",
    "\ttest_config.test_config_json  = f'checkpoints/{test_config.checkpoint_prefix}-config.json'\n",
    "\t\t\t\t\n",
    "\t# Model properties\n",
    "\ttest_config.conv_layers = [\n",
    "\t\t{\n",
    "\t\t\t'out': 6,\n",
    "\t\t\t'kernel': (3, 5),\n",
    "\t\t\t'pool': (1, 2),\n",
    "\t\t\t'dropout': 0.001\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t'out': 12,\n",
    "\t\t\t'kernel': (3, 3),\n",
    "\t\t\t'pool': (2, 2),\n",
    "\t\t\t'dropout': 0.001\n",
    "\t\t},\n",
    "\t]\n",
    "\n",
    "\ttest_config.dense_layers = [\n",
    "\t\t{\n",
    "\t\t\t'count': 64,\n",
    "\t\t\t'dropout': 0\n",
    "\t\t},\n",
    "\t]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary all persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable / disable config\n",
    "BINARY_ALL = False\n",
    "\n",
    "if BINARY_ALL:\n",
    "\ttest_config = dotdict()\n",
    "\t\n",
    "\t# Data selection\n",
    "\t# Whiat data to use (visual, audial, all together)\n",
    "\ttest_config.visual          = None\n",
    "\n",
    "\t# Set to True to enable binary classification mode\n",
    "\ttest_config.binary          = True\n",
    "\n",
    "\tif test_config.binary:\n",
    "\t\t\n",
    "\t\t# - - - B I N A R Y   O P T I O N S - - - \n",
    "\n",
    "\t\t# What person to use to train on\n",
    "\t\t# Set to None to train on each person\n",
    "\t\t# Set to number to train on single person\n",
    "\t\ttest_config.person          = 1\n",
    "\n",
    "\t\t# Set to True to combine data from all persons into single dataset\n",
    "\t\ttest_config.all_person_data = True\n",
    "\n",
    "\t\t# What phonemes to use during train\n",
    "\t\t# Set to None to train on each pair combination\n",
    "\t\t# Set to pair to train on this data pair\n",
    "\t\ttest_config.phoneme_classes = None\n",
    "\n",
    "\telse:\n",
    "\t\t\n",
    "\t\t# - - - M U L T I C L A S S   O P T I O N S - - - \n",
    "\n",
    "\t\t# List of persons to train on\n",
    "\t\t# Set to None to train on each person\n",
    "\t\t# Set to number to train on single person\n",
    "\t\ttest_config.person          = 1\n",
    "\n",
    "\t\t# Set to True to combine data from all persons into single dataset\n",
    "\t\ttest_config.all_person_data = False\n",
    "\n",
    "\t\t# Phoneme classes to use during train\n",
    "\t\t# Set to None to use all phonemes\n",
    "\t\ttest_config.phoneme_classes = None\n",
    "\n",
    "\t# Force enable/disable dataset reloading, for example, when performing multiple restarts\n",
    "\tRELOAD_DATASET = False\n",
    "\n",
    "\t# Train properties\n",
    "\ttest_config.test_size         = 0.2\n",
    "\ttest_config.batch_size        = 8\n",
    "\ttest_config.epochs            = 50\n",
    "\ttest_config.use_conv_sigmoid  = False\n",
    "\ttest_config.use_dense_sigmoid = False # test_config.binary\n",
    "\n",
    "\t# Train autoconfig\n",
    "\ttest_config.lr_step_size     = 40\n",
    "\ttest_config.lr_start         = 0.01\n",
    "\n",
    "\t# Transform properties\n",
    "\ttest_config.shift_transform_scale = 1.1\n",
    "\ttest_config.shift_transform_roll  = 1.0\n",
    "\ttest_config.noise_transform_scale = 0.002\n",
    "\n",
    "\ttest_config.seed       = 42\n",
    "\ttest_config.seed_steps = 1\n",
    "\ttest_config.num_workers = multiprocessing.cpu_count() // 4\n",
    "\n",
    "\t# Output properties\n",
    "\ttimestamp                     = round(time.time() * 1000)\n",
    "\ttest_config.checkpoint_prefix = f'autorun-{timestamp}'\n",
    "\ttest_config.test_json         = f'checkpoints/{test_config.checkpoint_prefix}.json'\n",
    "\ttest_config.test_config_json  = f'checkpoints/{test_config.checkpoint_prefix}-config.json'\n",
    "\t\t\t\t\n",
    "\t# Model properties\n",
    "\ttest_config.conv_layers = [\n",
    "\t\t{\n",
    "\t\t\t'out': 8,\n",
    "\t\t\t'kernel': (3, 5),\n",
    "\t\t\t'pool': (1, 2),\n",
    "\t\t\t'dropout': 0.001\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t'out': 16,\n",
    "\t\t\t'kernel': (3, 3),\n",
    "\t\t\t'pool': (2, 2),\n",
    "\t\t\t'dropout': 0.001\n",
    "\t\t},\n",
    "\t]\n",
    "\n",
    "\ttest_config.dense_layers = [\n",
    "\t\t{\n",
    "\t\t\t'count': 128,\n",
    "\t\t\t'dropout': 0\n",
    "\t\t},\n",
    "\t]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass single person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable / disable config\n",
    "MULTICLASS_SINGLE = False\n",
    "\n",
    "if MULTICLASS_SINGLE:\n",
    "\ttest_config = dotdict()\n",
    "\n",
    "\t# Data selection\n",
    "\t# Whiat data to use (visual, audial, all together)\n",
    "\ttest_config.visual          = None\n",
    "\n",
    "\t# Set to True to enable binary classification mode\n",
    "\ttest_config.binary          = False\n",
    "\n",
    "\tif test_config.binary:\n",
    "\t\t\n",
    "\t\t# - - - B I N A R Y   O P T I O N S - - - \n",
    "\n",
    "\t\t# What person to use to train on\n",
    "\t\t# Set to None to train on each person\n",
    "\t\t# Set to number to train on single person\n",
    "\t\ttest_config.person          = 1\n",
    "\n",
    "\t\t# Set to True to combine data from all persons into single dataset\n",
    "\t\ttest_config.all_person_data = True\n",
    "\n",
    "\t\t# What phonemes to use during train\n",
    "\t\t# Set to None to train on each pair combination\n",
    "\t\t# Set to pair to train on this data pair\n",
    "\t\ttest_config.phoneme_classes = None\n",
    "\n",
    "\telse:\n",
    "\t\t\n",
    "\t\t# - - - M U L T I C L A S S   O P T I O N S - - - \n",
    "\n",
    "\t\t# List of persons to train on\n",
    "\t\t# Set to None to train on each person\n",
    "\t\t# Set to number to train on single person\n",
    "\t\ttest_config.person          = None\n",
    "\n",
    "\t\t# Set to True to combine data from all persons into single dataset\n",
    "\t\ttest_config.all_person_data = False\n",
    "\n",
    "\t\t# Phoneme classes to use during train\n",
    "\t\t# Set to None to use all phonemes\n",
    "\t\ttest_config.phoneme_classes = None\n",
    "\n",
    "\tif True:\n",
    "\t\t\n",
    "\t\t# Force enable/disable dataset reloading, for example, when performing multiple restarts\n",
    "\t\tRELOAD_DATASET = True\n",
    "\n",
    "\t\t# Train properties\n",
    "\t\ttest_config.test_size         = 0.2\n",
    "\t\ttest_config.batch_size        = 8\n",
    "\t\ttest_config.epochs            = 50\n",
    "\t\ttest_config.use_conv_sigmoid  = False\n",
    "\t\ttest_config.use_dense_sigmoid = False # test_config.binary\n",
    "\n",
    "\t\t# Train autoconfig\n",
    "\t\ttest_config.lr_step_size     = 40\n",
    "\t\ttest_config.lr_start         = 0.01\n",
    "\n",
    "\t\t# Transform properties\n",
    "\t\ttest_config.shift_transform_scale = 1.1\n",
    "\t\ttest_config.shift_transform_roll  = 1.0\n",
    "\t\ttest_config.noise_transform_scale = 0.002\n",
    "\n",
    "\t\ttest_config.seed       = 42\n",
    "\t\ttest_config.seed_steps = 1\n",
    "\t\ttest_config.num_workers = multiprocessing.cpu_count() // 4\n",
    "\n",
    "\t\t# Output properties\n",
    "\t\ttimestamp                     = round(time.time() * 1000)\n",
    "\t\ttest_config.checkpoint_prefix = f'autorun-{timestamp}'\n",
    "\t\ttest_config.test_json         = f'checkpoints/{test_config.checkpoint_prefix}.json'\n",
    "\t\ttest_config.test_config_json  = f'checkpoints/{test_config.checkpoint_prefix}-config.json'\n",
    "\n",
    "\t# Model properties\n",
    "\ttest_config.conv_layers = [\n",
    "\t\t{\n",
    "\t\t\t'out': 4,\n",
    "\t\t\t'kernel': (1, 4),\n",
    "\t\t\t'pool': (1, 4),\n",
    "\t\t\t'dropout': 0.001\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t'out': 8,\n",
    "\t\t\t'kernel': (3, 5),\n",
    "\t\t\t'pool': (1, 2),\n",
    "\t\t\t'dropout': 0.001\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t'out': 16,\n",
    "\t\t\t'kernel': (3, 3),\n",
    "\t\t\t'pool': (2, 2),\n",
    "\t\t\t'dropout': 0.001\n",
    "\t\t},\n",
    "\t]\n",
    "\n",
    "\ttest_config.dense_layers = [\n",
    "\t\t\t{\n",
    "\t\t\t\t'count': 128,\n",
    "\t\t\t\t'dropout': 0\n",
    "\t\t\t},\n",
    "\t\t]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass all persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable / disable config\n",
    "MULTICLASS_ALL = False\n",
    "\n",
    "if MULTICLASS_ALL:\n",
    "\ttest_config = dotdict()\n",
    "\n",
    "\t# Data selection\n",
    "\t# Whiat data to use (visual, audial, all together)\n",
    "\ttest_config.visual          = None\n",
    "\n",
    "\t# Set to True to enable binary classification mode\n",
    "\ttest_config.binary          = False\n",
    "\n",
    "\tif test_config.binary:\n",
    "\t\t\n",
    "\t\t# - - - B I N A R Y   O P T I O N S - - - \n",
    "\n",
    "\t\t# What person to use to train on\n",
    "\t\t# Set to None to train on each person\n",
    "\t\t# Set to number to train on single person\n",
    "\t\ttest_config.person          = 1\n",
    "\n",
    "\t\t# Set to True to combine data from all persons into single dataset\n",
    "\t\ttest_config.all_person_data = True\n",
    "\n",
    "\t\t# What phonemes to use during train\n",
    "\t\t# Set to None to train on each pair combination\n",
    "\t\t# Set to pair to train on this data pair\n",
    "\t\ttest_config.phoneme_classes = None\n",
    "\n",
    "\telse:\n",
    "\t\t\n",
    "\t\t# - - - M U L T I C L A S S   O P T I O N S - - - \n",
    "\n",
    "\t\t# List of persons to train on\n",
    "\t\t# Set to None to train on each person\n",
    "\t\t# Set to number to train on single person\n",
    "\t\ttest_config.person          = None\n",
    "\n",
    "\t\t# Set to True to combine data from all persons into single dataset\n",
    "\t\ttest_config.all_person_data = True\n",
    "\n",
    "\t\t# Phoneme classes to use during train\n",
    "\t\t# Set to None to use all phonemes\n",
    "\t\ttest_config.phoneme_classes = None\n",
    "\n",
    "\tif True:\n",
    "\t\t\n",
    "\t\t# Force enable/disable dataset reloading, for example, when performing multiple restarts\n",
    "\t\tRELOAD_DATASET = False\n",
    "\n",
    "\t\t# Train properties\n",
    "\t\ttest_config.test_size         = 0.1\n",
    "\t\ttest_config.batch_size        = 16\n",
    "\t\ttest_config.epochs            = 80\n",
    "\t\ttest_config.use_conv_sigmoid  = False\n",
    "\t\ttest_config.use_dense_sigmoid = False # test_config.binary\n",
    "\n",
    "\t\t# Train autoconfig\n",
    "\t\ttest_config.lr_step_size     = 40\n",
    "\t\ttest_config.lr_start         = 0.01\n",
    "\n",
    "\t\t# Transform properties\n",
    "\t\ttest_config.shift_transform_scale = 1.1\n",
    "\t\ttest_config.shift_transform_roll  = 1.0\n",
    "\t\ttest_config.noise_transform_scale = 0.002\n",
    "\n",
    "\t\ttest_config.seed       = 42\n",
    "\t\ttest_config.seed_steps = 1\n",
    "\t\ttest_config.num_workers = multiprocessing.cpu_count() // 4\n",
    "\n",
    "\t\t# Output properties\n",
    "\t\ttimestamp                     = round(time.time() * 1000)\n",
    "\t\ttest_config.checkpoint_prefix = f'autorun-{timestamp}'\n",
    "\t\ttest_config.test_json         = f'checkpoints/{test_config.checkpoint_prefix}.json'\n",
    "\t\ttest_config.test_config_json  = f'checkpoints/{test_config.checkpoint_prefix}-config.json'\n",
    "\n",
    "\t# Model properties\n",
    "\ttest_config.conv_layers = [\n",
    "\t\t{\n",
    "\t\t\t'out': 4,\n",
    "\t\t\t'kernel': (1, 4),\n",
    "\t\t\t'pool': (1, 4),\n",
    "\t\t\t'dropout': 0.001\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t'out': 8,\n",
    "\t\t\t'kernel': (3, 5),\n",
    "\t\t\t'pool': (1, 2),\n",
    "\t\t\t'dropout': 0.001\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t'out': 16,\n",
    "\t\t\t'kernel': (3, 3),\n",
    "\t\t\t'pool': (2, 2),\n",
    "\t\t\t'dropout': 0.001\n",
    "\t\t},\n",
    "\t]\n",
    "\n",
    "\ttest_config.dense_layers = [\n",
    "\t\t{\n",
    "\t\t\t'count': 256,\n",
    "\t\t\t'dropout': 0\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t'count': 128,\n",
    "\t\t\t'dropout': 0\n",
    "\t\t},\n",
    "\t]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELOAD_DATASET         = True\n",
    "GLOBAL_LAST_CHECKPOINT = False\n",
    "SAVE_CHECKPOINTS       = True\n",
    "SAVE_CONFIG            = True\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\trun_test()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
